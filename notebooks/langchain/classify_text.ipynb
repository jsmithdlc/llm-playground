{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlixwDM48DuybPG4ATFYjL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jsmithdlc/llm-playground/blob/main/notebooks/langchain/classify_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YVNt-DRjsigA"
      },
      "outputs": [],
      "source": [
        "# Classify Text"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain langchain-openai"
      ],
      "metadata": {
        "id": "Ugcl7OdytDME",
        "outputId": "6bce23fe-e38f-4c12-aff1-0c6e52d92f1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.6/1.0 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/60.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credential for using openai key"
      ],
      "metadata": {
        "id": "PBzE0Kl8tQvi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(prompt = \"OpenAI key:\")"
      ],
      "metadata": {
        "id": "jn36KPJQtO0G",
        "outputId": "ca72fe62-8a59-4aaa-e600-1c913fbd59cc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI key:··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "llm = init_chat_model(\"gpt-4o-mini\", model_provider=\"openai\")"
      ],
      "metadata": {
        "id": "dXTU0Veztm9C"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pydantic model for tagging"
      ],
      "metadata": {
        "id": "Sn4GwN9jt_tD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_openai import ChatOpenAI\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "tagging_prompt = ChatPromptTemplate.from_template(\n",
        "\"\"\"\n",
        "Extract the desired information from the following passage.\n",
        "\n",
        "Only extract the properties mentioned in the 'Classification' function.\n",
        "\n",
        "Passage:\n",
        "{input}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "\n",
        "class Classification(BaseModel):\n",
        "    sentiment: str = Field(description=\"The sentiment of the text\")\n",
        "    aggressiveness: int = Field(\n",
        "        description=\"How aggressive the text is on a scale from 1 to 10\"\n",
        "    )\n",
        "    language: str = Field(description=\"The language the text is written in\")\n",
        "\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\").with_structured_output(\n",
        "    Classification\n",
        ")"
      ],
      "metadata": {
        "id": "S6r8F0UYt3lY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invoking model with nice spanish text"
      ],
      "metadata": {
        "id": "xoZb1nCJuU0n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\n",
        "prompt = tagging_prompt.invoke({\"input\": inp})\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "response"
      ],
      "metadata": {
        "id": "APRBdNcruLxB",
        "outputId": "0d72936b-410e-44fe-99d2-5e572e3efeb3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classification(sentiment='positive', aggressiveness=1, language='Spanish')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now with agressive text, note the almost upper limit of agressiveness"
      ],
      "metadata": {
        "id": "VuxXQevlueoh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"Te voy a matar malnacido\"\n",
        "prompt = tagging_prompt.invoke({\"input\": inp})\n",
        "response = llm.invoke(prompt)\n",
        "\n",
        "response.model_dump()"
      ],
      "metadata": {
        "id": "gFbYkKZxuYIQ",
        "outputId": "e848ec9e-dd2a-440e-e620-23f75123ae6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'sentiment': 'negative', 'aggressiveness': 9, 'language': 'Spanish'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finer Control\n",
        "\n",
        "We can dictate the specific categories in which the model should classify our text."
      ],
      "metadata": {
        "id": "YGRMA_AyvG8-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Classification(BaseModel):\n",
        "    sentiment: str = Field(..., enum=[\"happy\", \"neutral\", \"sad\", \"angry\"])\n",
        "    aggressiveness: int = Field(\n",
        "        ...,\n",
        "        description=\"describes how aggressive the statement is, the higher the number the more aggressive\",\n",
        "        enum=[1, 2, 3, 4, 5],\n",
        "    )\n",
        "    language: str = Field(\n",
        "        ..., enum=[\"spanish\", \"english\", \"french\", \"german\", \"italian\"]\n",
        "    )"
      ],
      "metadata": {
        "id": "L-FtAdH3ugbk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tagging_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "Extract the desired information from the following passage.\n",
        "\n",
        "Only extract the properties mentioned in the 'Classification' function.\n",
        "\n",
        "Passage:\n",
        "{input}\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "llm = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\").with_structured_output(\n",
        "    Classification\n",
        ")"
      ],
      "metadata": {
        "id": "tq6KFD8GvcOi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now answers will be restricted"
      ],
      "metadata": {
        "id": "K_URw-GmvpOv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"Estoy increiblemente contento de haberte conocido! Creo que seremos muy buenos amigos!\"\n",
        "prompt = tagging_prompt.invoke({\"input\": inp})\n",
        "llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "CK-C_DEzvlfn",
        "outputId": "4501add7-81bc-49e9-a091-925621a9b399",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classification(sentiment='happy', aggressiveness=1, language='spanish')"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"Estoy muy enojado con vos! Te voy a dar tu merecido!\"\n",
        "prompt = tagging_prompt.invoke({\"input\": inp})\n",
        "llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "4YHkPud_vrli",
        "outputId": "3787c025-f6c9-43ed-b54c-e6e1ffa80804",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classification(sentiment='angry', aggressiveness=4, language='spanish')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp = \"Weather is ok here, I can go outside without much more than a coat\"\n",
        "prompt = tagging_prompt.invoke({\"input\": inp})\n",
        "llm.invoke(prompt)"
      ],
      "metadata": {
        "id": "7hx1BSgIvvY0",
        "outputId": "4be33a32-b255-4949-bb20-f383d52b46bd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Classification(sentiment='happy', aggressiveness=1, language='english')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T8Zefuauv1d4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}